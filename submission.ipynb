{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df899e6d-025d-4b1b-a0fb-1a0ae9c43a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuf Submission- Internship submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c8401-4272-47c3-b0c5-7b7856ffe11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uploading the slected data from opus amd extraction. I have selected english french language pair\n",
    "\n",
    "# !unzip en-fr.txt.zip\n",
    "# !wc -l Op*\n",
    "# 216418--raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431c26b4-a591-4f7e-8a16-6a93c62ca7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  cleaning the data\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def write_file(file_path, lines):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "def remove_duplicates(source_lines, target_lines):\n",
    "    combined = list(set(zip(source_lines, target_lines)))\n",
    "    unique_source, unique_target = zip(*combined)\n",
    "    return list(unique_source), list(unique_target)\n",
    "\n",
    "def filter_by_length(source_lines, target_lines, max_words=200):\n",
    "    filtered_source, filtered_target = [], []\n",
    "    for source, target in zip(source_lines, target_lines):\n",
    "        if len(source.split()) <= max_words and len(target.split()) <= max_words:\n",
    "            filtered_source.append(source)\n",
    "            filtered_target.append(target)\n",
    "    return filtered_source, filtered_target\n",
    "\n",
    "def filter_by_ratio(source_lines, target_lines, ratio_threshold=1.5):\n",
    "    filtered_source, filtered_target = [], []\n",
    "    for source, target in zip(source_lines, target_lines):\n",
    "        source_len = len(source.split())\n",
    "        target_len = len(target.split())\n",
    "        if source_len > 0 and target_len > 0:\n",
    "            if (source_len / target_len <= ratio_threshold) and (target_len / source_len <= ratio_threshold):\n",
    "                filtered_source.append(source)\n",
    "                filtered_target.append(target)\n",
    "    return filtered_source, filtered_target\n",
    "\n",
    "def clean_files(source_file, target_file, output_source_file, output_target_file):\n",
    "    source_lines = read_file(source_file)\n",
    "    target_lines = read_file(target_file)\n",
    "\n",
    "    assert len(source_lines) == len(target_lines), \"Source and target files must have the same number of lines.\"\n",
    "\n",
    "    source_lines, target_lines = remove_duplicates(source_lines, target_lines)\n",
    "    source_lines, target_lines = filter_by_length(source_lines, target_lines)\n",
    "    source_lines, target_lines = filter_by_ratio(source_lines, target_lines)\n",
    "\n",
    "    write_file(output_source_file, source_lines)\n",
    "    write_file(output_target_file, target_lines)\n",
    "\n",
    "clean_files('OpenSubtitles.en-fr.en', 'OpenSubtitles.en-fr.fr', 'cleaned_text.en', 'cleaned_text.fr')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0d2b13-d08c-4890-9c73-d2e712195248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After cleaning-141711\n",
    "# !wc -l clean*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b883af-452b-4ed8-8513-8bed58790376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  splitting the test data and the train+development (we split later) data\n",
    "\n",
    "import random\n",
    "\n",
    "def split_train_test(source_lines, target_lines, train_ratio=0.98):\n",
    "    assert len(source_lines) == len(target_lines), \"Source and target lists must have the same number of lines.\"\n",
    "    combined = list(zip(source_lines, target_lines))\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    train_size = int(len(combined) * train_ratio)\n",
    "\n",
    "    train_data = combined[:train_size]\n",
    "    test_data = combined[train_size:]\n",
    "\n",
    "    train_source, train_target = zip(*train_data)\n",
    "    test_source, test_target = zip(*test_data)\n",
    "\n",
    "    return list(train_source), list(train_target), list(test_source), list(test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2ef174-a83a-4c08-a3b0-2f0953b96f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we have 2835 test sentences\n",
    "source_lines = read_file('cleaned_text.en')\n",
    "target_lines = read_file('cleaned_text.fr')\n",
    "train_src, train_trg, test_src, test_trg = split_train_test(source_lines, target_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d11e0d3-f206-41ca-8f96-cc5451bea805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a0f5e6-6966-49db-8fd2-db6b0595f761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_50k_sentences(train_src, train_trg, test_src):\n",
    "    vectorizer = CountVectorizer().fit(train_src + test_src)\n",
    "\n",
    "    train_vectors = vectorizer.transform(train_src)\n",
    "    test_vectors = vectorizer.transform(test_src)\n",
    "\n",
    "    # Compute cosine similarity between each training sentence and all test sentences\n",
    "    similarities = cosine_similarity(train_vectors, test_vectors)\n",
    "\n",
    "    max_similarities = np.max(similarities, axis=1)\n",
    "\n",
    "    # Get the indices of the top 50,000 most similar training sentences\n",
    "    top_50k_indices = np.argsort(max_similarities)[-50000:]\n",
    "\n",
    "    top_train_src = [train_src[i] for i in top_50k_indices]\n",
    "    top_train_trg = [train_trg[i] for i in top_50k_indices]\n",
    "\n",
    "    return top_train_src, top_train_trg\n",
    "\n",
    "\n",
    "\n",
    "top_train_src, top_train_trg = get_top_50k_sentences(train_src, train_trg, test_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae762ccc-b3ac-4d56-b881-859a9aa34c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_train_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64e234e-7eab-4b7b-874a-be3c4e8be68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: evaluate in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: transformers==4.33.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers[sentencepiece]==4.33.0) (4.33.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.33.0->transformers[sentencepiece]==4.33.0) (4.66.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers[sentencepiece]==4.33.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers[sentencepiece]==4.33.0) (4.25.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.33.0->transformers[sentencepiece]==4.33.0) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: sentence-transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from faiss-cpu) (2.1.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from faiss-cpu) (21.3)\n",
      "Collecting transformers<5.0.0,>=4.38.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->faiss-cpu) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.38.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.0\n",
      "    Uninstalling transformers-4.33.0:\n",
      "      Successfully uninstalled transformers-4.33.0\n",
      "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n",
      "Requirement already satisfied: sacrebleu in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.4.3)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (2024.7.24)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (2.1.2)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]==4.33.0\n",
    "\n",
    "!pip install accelerate -U\n",
    "\n",
    "!pip install faiss-cpu sentence-transformers\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a099e615-4889-4c26-ae96-6c247d79d651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading Nllb model\n",
    "Source_list=top_train_src\n",
    "Target_list=top_train_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2b8182-4327-469b-a74d-c8473b7dc19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    "\n",
    "# Initialize the dictionary with structure for 'id' and 'translation'\n",
    "my_dict = {'id': [], 'translation': []}\n",
    "count = 0\n",
    "\n",
    "for key, value in zip(Source_list, Target_list):\n",
    "    # Assuming key and value do not require stripping newlines\n",
    "    # Add the count and corresponding translation to the dictionary\n",
    "    my_dict['id'].append(count)\n",
    "    my_dict['translation'].append({'en': key, 'fr': value})\n",
    "    count += 1\n",
    "\n",
    "# Create the dataset from the dictionary\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset, validation_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# Combine the training and validation sets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9dceb0-1ec4-46e0-b7c8-43177e022327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_input_length = 150\n",
    "max_target_length = 150\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix=\"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "146ced0b-a6a6-49c5-870b-0b034503e3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "562e12ae-cb8a-4f12-836b-c5f4808389a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17726554-587a-49bc-a746-3f387f8fa381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bcd9dd0-14c4-4983-b205-0bce1a86ae0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.src_lang = \"eng_Latn\"  # English\n",
    "tokenizer.tgt_lang = \"fra_Latn\"  # French\n",
    "# tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeb41338-149d-4f66-9690-fc73efbb9502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4426c0-065f-416f-b14b-68c3264a68e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e12f3214-a1dd-4910-bd29-86bbd895ab10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42cadb48-f9ca-48b3-b539-71bcc28b8810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db518e17-1ba2-4726-8ab6-50f9ac800b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate import DataLoaderConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5281270-27d8-44b8-b4a8-c179e8c0e139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b375bedd-85b0-4484-910f-d6e75e1fc46f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7030' max='7030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7030/7030 2:03:08, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.251366</td>\n",
       "      <td>31.018300</td>\n",
       "      <td>11.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.244833</td>\n",
       "      <td>31.128800</td>\n",
       "      <td>11.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.243366</td>\n",
       "      <td>31.521400</td>\n",
       "      <td>11.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.243547</td>\n",
       "      <td>31.454200</td>\n",
       "      <td>11.642800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7030, training_loss=0.5549357303006212, metrics={'train_runtime': 7389.091, 'train_samples_per_second': 30.45, 'train_steps_per_second': 0.951, 'total_flos': 2.7734469253791744e+16, 'train_loss': 0.5549357303006212, 'epoch': 4.999111111111111})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cf2cc76-2dd7-4e91-b803-c7f61609e337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nllb-200-distilled-600M-finetuned-en-to-fr/checkpoint-5625'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c487b41c-9b3e-4599-a4c0-fba827948f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Specify the checkpoint directory or file path\n",
    "checkpoint_path = \"nllb-200-distilled-600M-finetuned-en-to-fr/checkpoint-5625//\"\n",
    "\n",
    "# Load the tokenizer from the checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "481047df-cd43-4747-9b9d-7648f1eafa41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store=[]\n",
    "for text in test_src:\n",
    "    # print(text)\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "    translated_tokens = model.generate(**tokenized_text,forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"))\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    # print(translated_text)\n",
    "    # count=count+1\n",
    "    store.append(translated_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76ebea0d-1c3f-42db-8c65-0fae62efc233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Untitled Folder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8265e81e-752e-423a-af06-7c7636c4960d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"base_outout\",\"w\") as ref_file:\n",
    "    for line in test_src:\n",
    "        line = str(line).strip()  \n",
    "        ref_file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1949dfd-1bbc-41ae-851b-318c41e5e2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2835 Usuf_outout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wc -l Usuf_outout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdeaead1-6e99-459b-a154-0a8ed156fc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 35.35533905932737\n",
      "TER score: 16.303410201851744\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def calculate_bleu_and_ter(hypotheses, references):\n",
    "    \n",
    "    references = [[ref] for ref in references]\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "    print(f\"BLEU score: {bleu.score}\")\n",
    "\n",
    "    ter = sacrebleu.corpus_ter(hypotheses, references)\n",
    "    print(f\"TER score: {ter.score}\")\n",
    "\n",
    "hypotheses = store\n",
    "references = test_trg\n",
    "calculate_bleu_and_ter(hypotheses, references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5078aa92-a15c-4149-aa63-0e985742cbe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0G\tnllb-200-distilled-600M-finetuned-en-to-fr/checkpoint-5625\n",
      "7.0G\tnllb-200-distilled-600M-finetuned-en-to-fr/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!du -h nllb-200-distilled-600M-finetuned-en-to-fr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1417b4-4b23-411a-9f86-bfa2e1e9ff27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
